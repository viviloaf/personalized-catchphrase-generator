{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "__author__ = 'maxim'\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching the text...\n",
      "Downloading data from https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt\n",
      "7544832/7540200 [==============================] - 16s 2us/step\n"
     ]
    }
   ],
   "source": [
    "print('\\nFetching the text...')\n",
    "url = 'https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt'\n",
    "path = tf.keras.utils.get_file('arxiv_abstracts.txt', origin=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the sentences...\n",
      "Num sentences: 7200\n"
     ]
    }
   ],
   "source": [
    "print('\\nPreparing the sentences...')\n",
    "max_sentence_len = 40\n",
    "with open(path) as file_:\n",
    "    docs = file_.readlines()\n",
    "\n",
    "# https://stackoverflow.com/a/40916306\n",
    "\n",
    "sentences = [[word for word in doc.lower().translate(str.maketrans('','',string.punctuation)).split()[:max_sentence_len]] for doc in docs]\n",
    "\n",
    "print('Num sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training word2vec...\n",
      "Result embedding shape: (1166, 100)\n",
      "Checking similar words:\n",
      "  model -> comprise (0.33), several (0.31), context (0.31), via (0.30), approach (0.29), training (0.29), lp (0.29), trains (0.29)\n",
      "  network -> networks (0.34), given (0.33), constrained (0.29), lies (0.28), trained (0.25), represent (0.24), be (0.24), from (0.23)\n",
      "  train -> based (0.39), eigendecompositions (0.34), extend (0.28), then (0.27), average (0.27), derive (0.27), adversarial (0.26), represent (0.26)\n",
      "  learn -> realize (0.38), units (0.34), lower (0.34), tend (0.34), automatically (0.33), best (0.32), relevant (0.31), enormous (0.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-0d52ff07dff8>:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  pretrained_weights = word_model.wv.syn0\n",
      "<ipython-input-24-0d52ff07dff8>:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.most_similar(word)[:8])\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining word2vec...')\n",
    "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "print('Checking similar words:')\n",
    "for word in ['model', 'network', 'train', 'learn']:\n",
    "    most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.most_similar(word)[:8])\n",
    "    print('  %s -> %s' % (word, most_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    return word_model.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "    return word_model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the data for LSTM...\n",
      "train_x shape: (7200, 40)\n",
      "train_y shape: (7200,)\n"
     ]
    }
   ],
   "source": [
    "print('\\nPreparing the data for LSTM...')\n",
    "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(sentences)], dtype=np.int32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "  for t, word in enumerate(sentence[:-1]):\n",
    "    train_x[i, t] = word2idx(word)\n",
    "  train_y[i] = word2idx(sentence[-1])\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM...\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining LSTM...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(units=emdedding_size))\n",
    "model.add(Dense(units=vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "  if temperature <= 0:\n",
    "    return np.argmax(preds)\n",
    "  preds = np.asarray(preds).astype('float64')\n",
    "  preds = np.log(preds) / temperature\n",
    "  exp_preds = np.exp(preds)\n",
    "  preds = exp_preds / np.sum(exp_preds)\n",
    "  probas = np.random.multinomial(1, preds, 1)\n",
    "  return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next(text, num_generated=10):\n",
    "  word_idxs = [word2idx(word) for word in text.lower().split()]\n",
    "  for i in range(num_generated):\n",
    "    prediction = model.predict(x=np.array(word_idxs))\n",
    "    idx = sample(prediction[-1], temperature=0.7)\n",
    "    word_idxs.append(idx)\n",
    "  return ' '.join(idx2word(idx) for idx in word_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "  print('\\nGenerating text after epoch: %d' % epoch)\n",
    "  texts = [\n",
    "    'deep convolutional',\n",
    "    'simple and effective',\n",
    "    'a nonconvex',\n",
    "    'a',\n",
    "  ]\n",
    "  for text in texts:\n",
    "    sample = generate_next(text)\n",
    "    print('%s... -> %s' % (text, sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "57/57 [==============================] - ETA: 0s - loss: 4.3269\n",
      "Generating text after epoch: 0\n",
      "deep convolutional... -> deep convolutional dnn timeconsuming path here variable distributions exponentially ranging of isometry\n",
      "simple and effective... -> simple and effective function fx theory structured important tuning transformation reaching up clustering\n",
      "a nonconvex... -> a nonconvex angular hierarchical noise enormous specifically schemes naturally concept promise understand\n",
      "a... -> a still overhead analysis revisit mdrnns recognized caused reconsidering respect presents\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 4.3269\n",
      "Epoch 2/20\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.8526\n",
      "Generating text after epoch: 1\n",
      "deep convolutional... -> deep convolutional stationary temporaldifference small take requires forcing nearly automatically structureactivityproperty obstacles\n",
      "simple and effective... -> simple and effective vast hypotheses postsynaptic richer combines defined convnets performing artificial gasfgadf\n",
      "a nonconvex... -> a nonconvex masks approximate lies other represent masking normalized better subject some\n",
      "a... -> a circumvent machines allow introduced grand sets structureactivityproperty perspective variety global\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.8502\n",
      "Epoch 3/20\n",
      "55/57 [===========================>..] - ETA: 0s - loss: 0.1726\n",
      "Generating text after epoch: 2\n",
      "deep convolutional... -> deep convolutional rectifier essential computable technique take exploding directly about process currently\n",
      "simple and effective... -> simple and effective generalizes to involves connected pseudoensemble allow spaces find tasks extend\n",
      "a nonconvex... -> a nonconvex open decomposition edge multivariate investigation datasets that analyze despite days\n",
      "a... -> a sequences small signal distributions been shown region formally affect open\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1711\n",
      "Epoch 4/20\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.0732\n",
      "Generating text after epoch: 3\n",
      "deep convolutional... -> deep convolutional consuming role methods infinite during significant regularizing patterns rigorously use\n",
      "simple and effective... -> simple and effective structured deep bayesian discuss adaptation universally area real partly denoising\n",
      "a nonconvex... -> a nonconvex computer call describing cluster or binary clear significant history partition\n",
      "a... -> a achievable multilayer reduce foundation procedure humans variants deeplysupervised extremely existing\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0731\n",
      "Epoch 5/20\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.0431\n",
      "Generating text after epoch: 4\n",
      "deep convolutional... -> deep convolutional main etc find customer svrg first same neuron higherorder variance\n",
      "simple and effective... -> simple and effective evolving lp systems offline training modelbased argue growing monro perform\n",
      "a nonconvex... -> a nonconvex see trained characterization nets machines does exploit boost analyze variable\n",
      "a... -> a risk variables that world significant typical perspective basic below analyze\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0430\n",
      "Epoch 6/20\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.0299\n",
      "Generating text after epoch: 5\n",
      "deep convolutional... -> deep convolutional builds however consequently unfortunately upper rise upper perspective dbns exploit\n",
      "simple and effective... -> simple and effective valuefunction variety revisit namely composition experiments directions that about at\n",
      "a nonconvex... -> a nonconvex pruning deep eg a projections ability space schemes three mdrnns\n",
      "a... -> a iii still what literature achievable form complicated theoretically extends adopted\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0299\n",
      "Epoch 7/20\n",
      "54/57 [===========================>..] - ETA: 0s - loss: 0.0224\n",
      "Generating text after epoch: 6\n",
      "deep convolutional... -> deep convolutional notion interference values automatic developed ph alignment structured designed estimating\n",
      "simple and effective... -> simple and effective net how attempt objective been procedures search modification machines machine\n",
      "a nonconvex... -> a nonconvex tcdnnblstmdnn thereby vanishing to domain that focus available ensemble vast\n",
      "a... -> a information powerful image fractions both reported yields python local higher\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0223\n",
      "Epoch 8/20\n",
      "53/57 [==========================>...] - ETA: 0s - loss: 0.0175\n",
      "Generating text after epoch: 7\n",
      "deep convolutional... -> deep convolutional customers mathematically hierarchy guaranteed customer distributed three convolutional algorithms shallow\n",
      "simple and effective... -> simple and effective considered argue by gradient language current der human recognized do\n",
      "a nonconvex... -> a nonconvex do steps gramian number paper optimize patterns same contribution picking\n",
      "a... -> a prediction estimated iterative perturbing each computes almost lateral versatility successes\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0174\n",
      "Epoch 9/20\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.0141\n",
      "Generating text after epoch: 8\n",
      "deep convolutional... -> deep convolutional nonconvex optimizing addressed differently performing generalizes convex labeled emerge exponential\n",
      "simple and effective... -> simple and effective calculating all proposed twolayer achievable summationdifference leverages spectrogram well information\n",
      "a nonconvex... -> a nonconvex forcing great normconstrained maxpooling suggesting that does clusterings pooling bound\n",
      "a... -> a perceptual are reason enormous subclass no fear notion case humans\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0141\n",
      "Epoch 10/20\n",
      "55/57 [===========================>..] - ETA: 0s - loss: 0.0117\n",
      "Generating text after epoch: 9\n",
      "deep convolutional... -> deep convolutional hf history cnns identify unit accuracy structure conjugate ph remaining\n",
      "simple and effective... -> simple and effective domain the outperform reconsidering solve have boltzmann both loss robbins\n",
      "a nonconvex... -> a nonconvex formalize domain paper 988 machine initializations requiring vanishing search protocol\n",
      "a... -> a of text net increasing examples led improve improvements of research\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0117\n",
      "Epoch 11/20\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.0099\n",
      "Generating text after epoch: 10\n",
      "deep convolutional... -> deep convolutional suffer der learningtrainingoptimization infinite reducing direct iterations constrained hessianfree acoustic\n",
      "simple and effective... -> simple and effective efficient restricted spawned approach shortcut usually rfn evolved like isometry\n",
      "a nonconvex... -> a nonconvex gprop tuned terms inverse customer characterization fx function achieved poor\n",
      "a... -> a sparse weights maximal outmatched for first separation now experimentally auxiliary\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0099\n",
      "Epoch 12/20\n",
      "55/57 [===========================>..] - ETA: 0s - loss: 0.0085\n",
      "Generating text after epoch: 11\n",
      "deep convolutional... -> deep convolutional algorithms encoding products remain accuracy valuefunction restarting moreover revisit exactly\n",
      "simple and effective... -> simple and effective minibatch focus direct recognizing few dropout explicit like tremendously possibly\n",
      "a nonconvex... -> a nonconvex combines processing opening up depth symmetries we connectivity more sequences\n",
      "a... -> a risk digit consistently propose while compact vision qsarqspr ffns followed\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0085\n",
      "Epoch 13/20\n",
      "53/57 [==========================>...] - ETA: 0s - loss: 0.0075\n",
      "Generating text after epoch: 12\n",
      "deep convolutional... -> deep convolutional values essentially image riskaverting neither plays fields kernel approximation put\n",
      "simple and effective... -> simple and effective approximate systems circumvent text be motivated easily neuron choice local\n",
      "a nonconvex... -> a nonconvex estimate experimentally independently start performance contain demand operates layerwise plays\n",
      "a... -> a operations speech hierarchical classical sum structure results and the on\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0074\n",
      "Epoch 14/20\n",
      "53/57 [==========================>...] - ETA: 0s - loss: 0.0065\n",
      "Generating text after epoch: 13\n",
      "deep convolutional... -> deep convolutional hierarchical settings approximation etc lowerbounded smooth second concept observed moreover\n",
      "simple and effective... -> simple and effective sequences consuming building adaptive sequence driven constrained by obtained between\n",
      "a nonconvex... -> a nonconvex neural net compared nor improves does still realize average factor\n",
      "a... -> a defined attention tasks construct poor tensor issues optimization protocol almost\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0065\n",
      "Epoch 15/20\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Generating text after epoch: 14\n",
      "deep convolutional... -> deep convolutional algorithms difficulties parameter maxpooling shape offers best many markov nature\n",
      "simple and effective... -> simple and effective picking similar domain firstorder angular proposed due system initializations recognition\n",
      "a nonconvex... -> a nonconvex insight good eigendecompositions during textitdeep need network demand users sum\n",
      "a... -> a improvement motivated rnns size neurons images confirmed textitdeep minimizes recognized\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0058\n",
      "Epoch 16/20\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.0052\n",
      "Generating text after epoch: 15\n",
      "deep convolutional... -> deep convolutional networks object since application customer depthdependency found computations rectified x\n",
      "simple and effective... -> simple and effective rely space effectively defined operates conjectured markov graphs believed steps\n",
      "a nonconvex... -> a nonconvex researchers reduced inference partition gradients contribution discrete which normalized stale\n",
      "a... -> a outmatched mutually formalize represent crucial how paper serving ffns allowing\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.0052\n",
      "Epoch 17/20\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.0047\n",
      "Generating text after epoch: 16\n",
      "deep convolutional... -> deep convolutional therefore analyze system digit users distributed research scalability dnns achieve\n",
      "simple and effective... -> simple and effective correct method estimates designing variables accurately especially continuous standard at\n",
      "a nonconvex... -> a nonconvex connections efficiently behind gatedfeedback based opening via lower achievable theoretical\n",
      "a... -> a address rnns sequence difficulties present outmatched each intensive often mass\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0047\n",
      "Epoch 18/20\n",
      "55/57 [===========================>..] - ETA: 0s - loss: 0.0042\n",
      "Generating text after epoch: 17\n",
      "deep convolutional... -> deep convolutional performance days valuefunction analyze churn number variants geometry model retaining\n",
      "simple and effective... -> simple and effective built basic directly suited lateral led dependence when extract summationdifference\n",
      "a nonconvex... -> a nonconvex serving speech smooth patterns exploration aims no periodically unclear px\n",
      "a... -> a gradientbased embeddings force caused complexity even approximation neurons grow near\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0042\n",
      "Epoch 19/20\n",
      "52/57 [==========================>...] - ETA: 0s - loss: 0.0039\n",
      "Generating text after epoch: 18\n",
      "deep convolutional... -> deep convolutional recognized improper starting seek is variation theano classification churn play\n",
      "simple and effective... -> simple and effective modification images sequences relevant learn difficulties variety clarifies tc calculating\n",
      "a nonconvex... -> a nonconvex rare most rnn able maxpooling predictive clarifies happens usually as\n",
      "a... -> a mdrnn dominated plays position events restricted especially multilayer attention digit\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0039\n",
      "Epoch 20/20\n",
      "55/57 [===========================>..] - ETA: 0s - loss: 0.0035\n",
      "Generating text after epoch: 19\n",
      "deep convolutional... -> deep convolutional can local consuming sequential language fundamental speeding comprise mass position\n",
      "simple and effective... -> simple and effective protocol determined easily challenging model case building ensemble under believed\n",
      "a nonconvex... -> a nonconvex clustering describing perspective depth padding protocol intertwined determined customer intermediate\n",
      "a... -> a almost power server effectiveness exploit speed new regions provide reasons\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f78c51519a0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
